{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c06b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #regular expressions -- searching text in document\n",
    "from nltk.corpus import stopwords #natural language tool kit\n",
    "from nltk.stem.porter import PorterStemmer #stemming is remove suffix and prefix of a word and return root word\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #convert text in feature vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b0bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GCV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3292a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = pd.read_csv('data/dataset.csv')\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882205d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168ab7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of missing values\n",
    "data_set.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae284c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null values with empty strings\n",
    "data_set = data_set.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc35a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Darrell LucusHouse Dem Aide: We Didn’t Even Se...\n",
       "1        Daniel J. FlynnFLYNN: Hillary Clinton, Big Wom...\n",
       "2        Consortiumnews.comWhy the Truth Might Get You ...\n",
       "3        Jessica Purkiss15 Civilians Killed In Single U...\n",
       "4        Howard PortnoyIranian woman jailed for fiction...\n",
       "                               ...                        \n",
       "20795    Jerome HudsonRapper T.I.: Trump a ’Poster Chil...\n",
       "20796    Benjamin HoffmanN.F.L. Playoffs: Schedule, Mat...\n",
       "20797    Michael J. de la Merced and Rachel AbramsMacy’...\n",
       "20798    Alex AnsaryNATO, Russia To Hold Parallel Exerc...\n",
       "20799               David SwansonWhat Keeps the F-35 Alive\n",
       "Name: content, Length: 20800, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mergin the author name and news title\n",
    "data_set['content'] = data_set['author'] + data_set['title']\n",
    "\n",
    "data_set['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe1d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating data and target\n",
    "data = data_set.drop(columns='label',axis=1)\n",
    "target = data_set['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50206525",
   "metadata": {},
   "source": [
    "Stemming--\n",
    "stemming is the process of reducing a word to its root word\n",
    "ex- acter, acting , actress --> act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa08d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9b7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0717c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['content'] = data_set['content'].apply(stemming)\n",
    "#print(data_set['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35945dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r   r   e   l   l   l   u   c   u   h   u   e   e   e   w   e   n   e   v   e   n   e   e   c   e   l   e   e   r   u   n   l   j   n   c   h   f   f   e   z   w   e   e   e'\n",
      " 'n   e   l   j   f   l   n   n   f   l   n   n   h   l   l   r   c   l   n   n   b   g   w   n   n   c   p   u   b   r   e   b   r'\n",
      " 'c   n   r   u   n   e   w   c   w   h   h   e   r   u   h   g   h   g   e   u   f   r   e'\n",
      " ...\n",
      " 'c   h   e   l   j   e   l   e   r   c   e   n   r   c   h   e   l   b   r   c   r   e   c   e   v   e   k   e   v   e   r   p   p   r   c   h   b   h   u   n   b   h   e   n   e   w   r   k   e'\n",
      " 'l   e   x   n   r   n   r   u   h   l   p   r   l   l   e   l   e   x   e   r   c   e   n   b   l   k   n'\n",
      " 'v   w   n   n   w   h   k   e   e   p   h   e   f   l   v   e']\n"
     ]
    }
   ],
   "source": [
    "data = data_set['content'].values\n",
    "target = data_set['label'].values\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4147b4d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#converting textual data to numerical data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2053\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> 2053\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1220\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1218\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1221\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1222\u001b[0m         )\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#converting textual data to numerical data\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data)\n",
    "\n",
    "data = vectorizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728350e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_target,test_target = train_test_split(data,target,test_size=0.2,stratify= target,random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the training data\n",
    "train_data_prediction = model.predict(train_data)\n",
    "training_data_accuracy = accuracy_score(train_data_prediction, train_target)\n",
    "\n",
    "print('accuracy score of trainig data : ',training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa043fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the test data\n",
    "test_data_prediction = model.predict(test_data)\n",
    "testing_data_accuracy = accuracy_score(test_data_prediction, test_target)\n",
    "\n",
    "print('accuracy score of testing data : ',testing_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = test_data[0]\n",
    "\n",
    "prediction = model.predict(new_data)\n",
    "\n",
    "if(prediction[0]==0):\n",
    "    print('the news is Real')\n",
    "\n",
    "else:\n",
    "    print('the news is Fake')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cff80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
